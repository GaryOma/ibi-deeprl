{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "import gym\n",
    "from gym import wrappers, logger\n",
    "\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# def blotzmann(qvalues, tau=0.01):\n",
    "#     prob = []\n",
    "#     for qvalue in qvalues:\n",
    "#         prob.append()\n",
    "#     print(\"lol\")\n",
    "\n",
    "def epsilongreedy(qvalues, epsilon=0.5):\n",
    "    _, max_ind = qvalues.max(0)\n",
    "    return max_ind.item() if np.random.random() < epsilon else np.random.randint(len(qvalues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(Net, self).__init__()\n",
    "        print(in_size, out_size)\n",
    "        self.in_l = nn.Linear(in_size, out_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.in_l(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, _net):\n",
    "        self.mem = deque(maxlen=10000)\n",
    "        self.net = _net\n",
    "\n",
    "    def choose_action(self, _observation, _reward, _done):\n",
    "        qvalues = self.qvalue(_observation)\n",
    "        print(qvalues)\n",
    "        action_ind = epsilongreedy(qvalues, epsilon=0.1)\n",
    "        return action_ind\n",
    "        # return self.action_space.sample()\n",
    "\n",
    "    def experience(self, _state, _action, _reward, _next_state, _done):\n",
    "        self.mem.append((_state, _action, _reward, _next_state, _done))\n",
    "\n",
    "    def minibatch(self, size=5):\n",
    "        batch_s = min(size, len(self.mem))\n",
    "        return random.sample(self.mem, batch_s)\n",
    "\n",
    "    def qvalue(self, state):\n",
    "        inp = torch.tensor(state).type(torch.FloatTensor)\n",
    "        # for w in self.W[:-1]:\n",
    "        #     # inp = torch.cat((torch.Tensor([1.0]), inp))\n",
    "        #     # inp = func(torch.mv(w, inp))\n",
    "        #     inp = 1/(1 + torch.exp(-torch.mv(w, inp)))\n",
    "        # return torch.mv(self.W[-1], inp.float())\n",
    "        return self.net(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Making new env: CartPole-v1\n",
      "INFO: Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "4 2\n"
     ]
    }
   ],
   "source": [
    "# You can set the level to logger.DEBUG or logger.WARN if you\n",
    "# want to change the amount of output.\n",
    "logger.set_level(logger.INFO)\n",
    "\n",
    "env = gym.make(env_name)\n",
    "\n",
    "# You provide the directory to write to (can be an existing\n",
    "# directory, including one with existing data -- all monitor files\n",
    "# will be namespaced). You can also dump to a tempdir if you'd\n",
    "# like: tempfile.mkdtemp().\n",
    "outdir = '/tmp/random-agent-results'\n",
    "env = wrappers.Monitor(env, directory=outdir, force=True, video_callable=False)\n",
    "env.seed(0)\n",
    "observation_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "net = Net(observation_size, env.action_space.n)\n",
    "agent = Agent(net)\n",
    "\n",
    "episode_count = 1000\n",
    "reward = 0\n",
    "done = False\n",
    "\n",
    "rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1720, 0.3718], grad_fn=<ReluBackward0>)\n",
      "tensor([0.3122, 0.2160], grad_fn=<ReluBackward0>)\n",
      "tensor([0.1757, 0.3670], grad_fn=<ReluBackward0>)\n",
      "tensor([0.0376, 0.5185], grad_fn=<ReluBackward0>)\n",
      "tensor([0.0000, 0.6713], grad_fn=<ReluBackward0>)\n",
      "tensor([0.0329, 0.5183], grad_fn=<ReluBackward0>)\n",
      "tensor([0.0000, 0.6738], grad_fn=<ReluBackward0>)\n",
      "tensor([0.0257, 0.5234], grad_fn=<ReluBackward0>)\n",
      "tensor([0.1614, 0.3734], grad_fn=<ReluBackward0>)\n",
      "tensor([0.0202, 0.5311], grad_fn=<ReluBackward0>)\n",
      "tensor([0.1554, 0.3821], grad_fn=<ReluBackward0>)\n",
      "tensor([0.0136, 0.5407], grad_fn=<ReluBackward0>)\n",
      "tensor([0.1482, 0.3927], grad_fn=<ReluBackward0>)\n",
      "tensor([0.2845, 0.2443], grad_fn=<ReluBackward0>)\n",
      "tensor([0.4228, 0.0949], grad_fn=<ReluBackward0>)\n",
      "tensor([0.2850, 0.2513], grad_fn=<ReluBackward0>)\n",
      "tensor([0.1457, 0.4075], grad_fn=<ReluBackward0>)\n",
      "tensor([0.0045, 0.5644], grad_fn=<ReluBackward0>)\n",
      "tensor([0.1396, 0.4148], grad_fn=<ReluBackward0>)\n",
      "tensor([0.2764, 0.2650], grad_fn=<ReluBackward0>)\n",
      "tensor([0.4152, 0.1142], grad_fn=<ReluBackward0>)\n",
      "tensor([0.2777, 0.2696], grad_fn=<ReluBackward0>)\n",
      "tensor([0.4174, 0.1167], grad_fn=<ReluBackward0>)\n",
      "tensor([0.5597, 0.0000], grad_fn=<ReluBackward0>)\n",
      "tensor([0.4260, 0.1127], grad_fn=<ReluBackward0>)\n",
      "tensor([0.5701, 0.0000], grad_fn=<ReluBackward0>)\n",
      "tensor([0.7171, 0.0000], grad_fn=<ReluBackward0>)\n",
      "tensor([0.8674, 0.0000], grad_fn=<ReluBackward0>)\n",
      "tensor([1.0215, 0.0000], grad_fn=<ReluBackward0>)\n",
      "tensor([0.9016, 0.0000], grad_fn=<ReluBackward0>)\n",
      "tensor([1.0601, 0.0000], grad_fn=<ReluBackward0>)\n",
      "tensor([1.2226, 0.0000], grad_fn=<ReluBackward0>)\n",
      "INFO: Finished writing results. You can upload them to the scoreboard via gym.upload('/tmp/random-agent-results')\n"
     ]
    }
   ],
   "source": [
    "for i in range(episode_count):\n",
    "    ob = env.reset()\n",
    "    rewardSum = 0\n",
    "    steps = 0\n",
    "    while not done:\n",
    "        action = agent.choose_action(ob, reward, done)\n",
    "        current_state = ob\n",
    "        ob, reward, done, _ = env.step(action)\n",
    "        rewardSum += reward\n",
    "        steps += 1\n",
    "\n",
    "        # Experience replay\n",
    "        agent.experience(current_state, action, reward, ob, done)\n",
    "        minibatch = agent.minibatch(size=10)\n",
    "        if done:\n",
    "            rewards.append(rewardSum)\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/home/gary/.virtualenvs/deeprl/bin/python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "deeprl",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "deeprl"
  },
  "name": "experience_replay.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
