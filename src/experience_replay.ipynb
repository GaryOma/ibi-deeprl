{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "import gym\n",
    "from gym import wrappers, logger\n",
    "\n",
    "import random\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Finished writing results. You can upload them to the scoreboard via gym.upload('/tmp/random-agent-results')\n"
     ]
    }
   ],
   "source": [
    "class Agent:\n",
    "    def __init__(self, observation_size, action_space):\n",
    "        self.mem = deque(maxlen=10000)\n",
    "        self.action_space = action_space\n",
    "        self.observation_size = observation_size\n",
    "        print(action_space)\n",
    "        layer_sizes = [\n",
    "            self.observation_size,\n",
    "            self.action_space.n\n",
    "        ]\n",
    "\n",
    "        self.W = []\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.W.append(torch.ones(layer_sizes[i + 1], layer_sizes[i]).type(torch.FloatTensor))\n",
    "\n",
    "    def choose_action(self, _observation, _reward, _done):\n",
    "        return self.action_space.sample()\n",
    "\n",
    "    def experience(self, _state, _action, _reward, _next_state, _done):\n",
    "        self.mem.append((_state, _action, _reward, _next_state, _done))\n",
    "\n",
    "    def minibatch(self, size=5):\n",
    "        batch_s = min(size, len(self.mem))\n",
    "        return random.sample(self.mem, batch_s)\n",
    "\n",
    "    def qvalue(self, state):\n",
    "        inp = torch.tensor(state.T).type(torch.FloatTensor)\n",
    "        for w in self.W[:-1]:\n",
    "            # inp = torch.cat((torch.Tensor([1.0]), inp))\n",
    "            # inp = func(torch.mv(w, inp))\n",
    "            inp = 1/(1 + torch.exp(-torch.mv(w, inp)))\n",
    "        print(inp)\n",
    "        print(self.W[-1])\n",
    "        Y = torch.mv(self.W[-1], inp.double())\n",
    "        print(Y)\n",
    "        # e2 = (label.squeeze() - Y)\n",
    "        # quad = e2.pow(2).sum()/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Making new env: CartPole-v1\n",
      "INFO: Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "# You can set the level to logger.DEBUG or logger.WARN if you\n",
    "# want to change the amount of output.\n",
    "logger.set_level(logger.INFO)\n",
    "\n",
    "env = gym.make(env_name)\n",
    "\n",
    "# You provide the directory to write to (can be an existing\n",
    "# directory, including one with existing data -- all monitor files\n",
    "# will be namespaced). You can also dump to a tempdir if you'd\n",
    "# like: tempfile.mkdtemp().\n",
    "outdir = '/tmp/random-agent-results'\n",
    "env = wrappers.Monitor(env, directory=outdir, force=True, video_callable=False)\n",
    "env.seed(0)\n",
    "observation_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "agent = Agent(observation_size, env.action_space)\n",
    "\n",
    "episode_count = 1000\n",
    "reward = 0\n",
    "done = False\n",
    "\n",
    "rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.04456399  0.04653909  0.01326909 -0.02099827]\n",
      "tensor([-0.0446,  0.0465,  0.0133, -0.0210])\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Float but got scalar type Double for argument #2 'vec' in call to _th_mv",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-196-908d40b89f53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mminibatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-193-a7a6801d8f5c>\u001b[0m in \u001b[0;36mqvalue\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# e2 = (label.squeeze() - Y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Float but got scalar type Double for argument #2 'vec' in call to _th_mv"
     ]
    }
   ],
   "source": [
    "for i in range(episode_count):\n",
    "    ob = env.reset()\n",
    "    rewardSum = 0\n",
    "    steps = 0\n",
    "    while not done:\n",
    "        action = agent.choose_action(ob, reward, done)\n",
    "        current_state = ob\n",
    "        ob, reward, done, _ = env.step(action)\n",
    "        rewardSum += reward\n",
    "        steps += 1\n",
    "\n",
    "        # Experience replay\n",
    "        agent.experience(current_state, action, reward, ob, done)\n",
    "        minibatch = agent.minibatch(size=10)\n",
    "        print(current_state)\n",
    "        agent.qvalue(current_state)\n",
    "\n",
    "        if done:\n",
    "            rewards.append(rewardSum)\n",
    "            break\n",
    "        # Note there's no env.render() here. But the environment still can open window and\n",
    "        # render if asked by env.monitor: it calls env.render('rgb_array') to record video.\n",
    "        # Video is not recorded every episode, see capped_cubic_video_schedule for details.\n",
    "\n",
    "# Close the env and write monitor result info to disk\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/home/gary/.virtualenvs/deeprl/bin/python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "deeprl",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "deeprl"
  },
  "name": "experience_replay.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
